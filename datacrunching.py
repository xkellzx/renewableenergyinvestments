# -*- coding: utf-8 -*-
"""DataCrunching.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eBh_Ey5IStfVN7UfzOjk6vwRy89jfwfi
"""

!pip install -U scikit-learn

from sklearn.ensemble import RandomForestClassifier
import numpy as np
import pandas as pd
import math
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import statistics
from sklearn.decomposition import PCA
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import Normalizer
from sklearn.metrics import silhouette_score
from sklearn.decomposition import PCA
import plotly.express as px

def clustering_by_label(dataset):
  clusters = []
  df = pd.DataFrame(dataset)
  labels = df.Label.unique()
  for label in labels:
    clusters.append(df.loc[df['Label'] == label])
  return clusters

def X_y_Split(dataset, column_begin, column_end, To_List):
  columns = list(dataset.columns)
  data_new_x = dataset[columns[column_begin:column_end]]
  data_new_y = dataset["TotalAmountofAssistance"].transpose()

  X = data_new_x
  y = data_new_y
 
  if To_List == True:
    X = data_new_x.values.tolist()
    y = data_new_y.values.tolist()
  


  return X, y

def Merge(X1, X2):
  mergeddf = X1.merge(X2, how='left', on = ["State", "Year"])
  return mergeddf

def RandomForestRMSE(X,y):
  diff_squared = []
  for n in range(6,len(X)+1,6):
    clf = RandomForestClassifier(random_state=0)
    training_x = X[n-6:n-1]

    testing_x = X[n-1]

    training_y = y[n-6:n-1]
    testing_y = y[n-1]

    clf.fit(training_x,training_y)
    pred_value = clf.predict([testing_x])
    diff_squared.append((float(pred_value)-float(testing_y))*(float(pred_value)-float(testing_y)))

  RMSE = math.sqrt(sum(diff_squared)/50)
  return RMSE

"""Working Sheets!

"""

#####Creating Datasets
dataset1 = pd.read_csv("/content/drive/MyDrive/Datathon23/investment.csv")
dataset1 = dataset1.sort_values(by = ["State", "Year"], ignore_index=True)

dataset2 = pd.read_csv("/content/drive/MyDrive/Datathon23/StateElectricity.csv")
dataset2 = dataset2.sort_values(by = ["State", "Year"], ignore_index=True)
dataset2 = dataset2[(dataset2.Year != 2020) & (dataset2.State != 'District of Columbia')].reset_index()

dataset3 = pd.read_csv("/content/drive/MyDrive/Datathon23/investmentwdisburse.csv")

test_data = pd.read_csv("/content/drive/MyDrive/Datathon23/invest2020.csv")

print(dataset1.columns)

X1, y = X_y_Split(dataset1, 1, -1, False)


dataset = Merge(X1, dataset2)
dataset = Merge(dataset, dataset3)

# print(Y_scaled)



kmeans = KMeans(n_clusters=5, random_state=0,n_init = "auto")
kfit = kmeans.fit(X_pca)

arr_of_labels = kmeans.labels_

output = np.array([])
for n in range(5,251,5):
  label = statistics.mode(arr_of_labels[n-5:n-1])
  state = np.array([label,label,label,label,label])
  output = np.concatenate((output,state))

print(output)

plt.scatter(X_pca[:, 0],
                X_pca[:, 1],
                color='#b4d2b1',
                alpha=0.9,
                marker="o"
                )


# Assign the cluster centers: centroids

labels = kmeans.predict(X_pca)
print(labels)
# plt.scatter(X,y,c=labels,alpha=0.5)

centroids = kmeans.cluster_centers_# Assign the columns of centroids: centroids_x, centroids_y
centroids_x = centroids[:,0]
centroids_y = centroids[:,1]# Make a scatter plot of centroids_x and centroids_y
plt.scatter(centroids_x,centroids_y,marker='x',s=50)
plt.show()

#######Standardizing

X = dataset
y = y

scalerx = StandardScaler().fit(X)
X_scaled = scalerx.transform(X)

scalery = StandardScaler().fit(y)
Y_scaled = scalery.transform(y)


# X = np.array(X_scaled)
# y = np.array(Y_scaled)

pca_scaled_std = PCA(n_components=2,random_state=0)
X_pca = pca_scaled_std.fit_transform(X_scaled)


RMSE = RandomForestRMSE(X_pca,y[0])

# print(RMSE)
# kmeans = KMeans(n_clusters=5, random_state=0, n_init="auto")

# kfit = kmeans.fit(X_pca)

# arr_of_labels = kmeans.labels_

# labels = np.array([])
# for n in range(5,251,5):
#   label = statistics.mode(arr_of_labels[n-5:n-1])
#   state = np.array([label,label,label,label,label])
#   labels = np.concatenate((labels,state))

###Training Data
dataset1 = pd.read_csv("/content/drive/MyDrive/Datathon23/investment.csv")
dataset1 = dataset1.sort_values(by = ["State", "Year"], ignore_index=True)

dataset2 = pd.read_csv("/content/drive/MyDrive/Datathon23/StateElectricity.csv")
dataset2 = dataset2.sort_values(by = ["State", "Year"], ignore_index=True)
dataset2 = dataset2[(dataset2.Year != 2020) & (dataset2.State != 'District of Columbia')].reset_index()

dataset3 = pd.read_csv("/content/drive/MyDrive/Datathon23/disbursement_train.csv")
dataset3 = dataset3.rename(columns={"sum_dis": 'Disbursement', "Fiscal.Year": "Year"})

dataset4 = pd.read_csv("/content/drive/MyDrive/Datathon23/gdp.csv")
dataset4 = dataset4.sort_values(by = ["State", "Year"], ignore_index=True)
dataset4 = dataset4[(dataset4.Year != 2020)].reset_index().drop(["index","Unnamed: 0"], axis = 1)


dataset5 = pd.read_csv("/content/drive/MyDrive/Datathon23/population.csv")
dataset5 = dataset5.sort_values(by = ["State", "Year"], ignore_index=True)
dataset5 = dataset5[(dataset5.Year != 2020)].reset_index().drop(["index","Unnamed: 0"], axis = 1)


cols = ['State', 'Year', 'BDPRP', 'BFFDB', 'CLPRB', 'CLPRK',
       'EMFDB', 'ENPRP', 'GETCB', 'HYTCB', 'NCPRB', 'NGMPB', 'NUETB', 'PAPRB',
       'REPRB', 'SOTCB', 'WWPRB', 'WYTCB', 'CO2.Emissions..Mmt.',
       'TotalNumberofInvestments', 
       'Average retail price', 'Net summer capacity', 'Net generation',
       'Total retail sales', 'GDP', 'Population', 'Disbursement', 'TotalAmountofAssistance']

dataset = Merge(dataset1, dataset2)
dataset = Merge(dataset, dataset4)
training_data = Merge(dataset, dataset5)

training_data = training_data.merge(dataset3, how = 'left', on = ["State", "Year"])

training_data = training_data.fillna(0)
training_data = training_data[cols]
training_data.to_csv('/content/training_data.csv')

####Creating testing data of 2020 with all parameters
test_data = test_data[test_data.Year== 2020] 
dataset2 = pd.read_csv("/content/drive/MyDrive/Datathon23/StateElectricity.csv")
dataset2 = dataset2.sort_values(by = ["State", "Year"], ignore_index=True)
dataset2 = dataset2[(dataset2.Year == 2020) & (dataset2.State != 'District of Columbia')].reset_index()

dataset4 = pd.read_csv("/content/drive/MyDrive/Datathon23/disbursement_test.csv")
dataset4 = dataset4.rename(columns={"sum_dis": 'Disbursement', "Fiscal.Year": "Year"})

dataset5 = pd.read_csv("/content/drive/MyDrive/Datathon23/gdp.csv")
dataset5 = dataset5.sort_values(by = ["State", "Year"], ignore_index=True)
dataset5 = dataset5[(dataset5.Year == 2020)].reset_index().drop(["index","Unnamed: 0"], axis = 1)


dataset6 = pd.read_csv("/content/drive/MyDrive/Datathon23/population.csv")
dataset6 = dataset6.sort_values(by = ["State", "Year"], ignore_index=True)
dataset6 = dataset6[(dataset6.Year == 2020)].reset_index().drop(["index","Unnamed: 0"], axis = 1)


test_set = Merge(test_data, dataset2)
test_set = Merge(test_set, dataset5)
test_set = Merge(test_set, dataset6)
test_set = test_set.merge(dataset4, how = 'left', on = ["State", "Year"])

test_set = test_set.fillna(0)


cols = ['State', 'Year', 'BDPRP', 'BFFDB', 'CLPRB', 'CLPRK',
       'EMFDB', 'ENPRP', 'GETCB', 'HYTCB', 'NCPRB', 'NGMPB', 'NUETB', 'PAPRB',
       'REPRB', 'SOTCB', 'WWPRB', 'WYTCB', 'CO2.Emissions..Mmt.',
       'TotalNumberofInvestments', 
       'Average retail price', 'Net summer capacity', 'Net generation',
       'Total retail sales', 'GDP', 'Population', 'Disbursement', 'TotalAmountofAssistance']

test_set = test_set[cols]
test_set.to_csv('/content/testing_data.csv')

full_set = pd.concat([training_data, test_set], ignore_index=True)
full_set = full_set.sort_values(by = ["State", "Year"], ignore_index=True)
full_set.to_csv('/content/training_and_testing_data.csv')

####TESTING
training_data = pd.read_csv("/content/training_and_testing_data (3).csv")
Xtest, ytest = X_y_Split(training_data, 0,-1, False)

Xtest = Xtest.drop(["State", "Year"], axis = 1)
ytest = np.array([y])

Xtest = Xtest.astype(float)

scalerx = StandardScaler().fit(Xtest)
X_scaled = scalerx.transform(Xtest)

pca_scaled_std = PCA(n_components=2,random_state=0)
X_pca = pca_scaled_std.fit_transform(X_scaled)

kmeans = KMeans(n_clusters=5, random_state=0, n_init = "auto")
kfit = kmeans.fit(X_pca)

arr_of_labels = kmeans.labels_

output = np.array([])
for n in range(5,251,5):
  label = statistics.mode(arr_of_labels[n-5:n-1])
  state = np.array([label,label,label,label,label])
  output = np.concatenate((output,state))

np.savetxt('state_labels.csv', output, delimiter=',')

####WORKING
training_data = pd.read_csv("/content/training_data (1).csv")
training_data["Label"] = output

cols = training_data.columns.tolist()
cols = ['Label', 'State', 'Year', 'BDPRP', 'BFFDB', 'CLPRB', 'CLPRK', 'EMFDB', 
        'ENPRP', 'GETCB', 'HYTCB', 'NCPRB', 'NGMPB', 'NUETB', 'PAPRB', 'REPRB', 
        'SOTCB', 'WWPRB', 'WYTCB', 'CO2.Emissions..Mmt.', 'TotalNumberofInvestments', 
        'Average retail price', 'Net summer capacity', 'Net generation', 
        'Total retail sales', 'GDP', 'Population', 'Disbursement', 'TotalAmountofAssistance']
training_data = training_data[cols]

X1, y = X_y_Split(training_data, 1, -1, False)

X = X1.drop(["State", "Year"], axis = 1)
y = np.array([y])

scalerx = StandardScaler().fit(X)
X_scaled = scalerx.transform(X)

pca_scaled_std = PCA(n_components=2,random_state=0)
X_pca = pca_scaled_std.fit_transform(X)

scalery = Normalizer().fit(y)
Y_scaled = scalery.transform(y)

silhouette_scores = []
for k in range(2, 10):
    km = KMeans(n_clusters=k, 
                max_iter=300, 
                tol=1e-04, 
                n_init='auto', 
                random_state=42)
    km.fit(X_pca)
    silhouette_scores.append(silhouette_score(X, km.labels_))

# fig, ax = plt.subplots()
# ax.plot(range(2, 11), silhouette_scores, 'bo-')
# ax.set_title('Silhouette Score Method with First Principle Component')
# ax.set_xlabel('Number of clusters')
# ax.set_ylabel('Silhouette Scores')
# plt.xticks(range(2, 11))
# plt.tight_layout()
# plt.show()

df = pd.DataFrame(dict(
    x = range(2,10),
    y = silhouette_scores
))


fig = px.line(df, x = "x", y = "y", title='Silhouette Score with First Principle Component', 
              labels={
                     "x": "Number of Clusters",
                     "y": "Silhouette Scores",
                 },
              markers=True)
fig.show()

df = pd.DataFrame(X_pca, columns = ["PC1", "PC2"])
df["Label"] = training_data[["Label"]]

# Assign the cluster centers: centroids
kmeans = KMeans(n_clusters=5, random_state=0, n_init = "auto")
kfit = kmeans.fit(X_pca)


centroids = kfit.cluster_centers_# Assign the columns of centroids: centroids_x, centroids_y
centroids_x = centroids[:,0]
centroids_y = centroids[:,1]# Make a scatter plot of centroids_x and centroids_y

plt.scatter(X_pca[:, 0],
                X_pca[:, 1],
                color='#B99BEA',
                alpha=0.9,
                marker="o"
                )
plt.scatter(centroids_x,centroids_y,marker='x',s=75, color = "#144996")
plt.title("PCA Data Distribution with Cluster Centers")
plt.xlabel("Principle Component 1")
plt.ylabel("Principle Component 2")
plt.show()


# print(df)
# f, ax = plt.subplots(figsize=(6.5, 6.5))
# sns.despine(f, left=True, bottom=True)
# sns.scatterplot(x="PC1", y="PC2",
#                 hue="Label", 
#                 palette="ch:r=-.2,d=.3_r",
#                 linewidth=0,
#                 data=df, ax=ax)
# sns.scatterplot(x=centroids_x, y=centroids_y,
#                 palette="ch:r=-.2,d=.3_r",
#                 linewidth=0, marker = 'x',ax=ax)

def translate(value):
  dick = {
    "Alabama": "AL",
    "Alaska": "AK",
    "Arizona": "AZ",
    "Arkansas": "AR",
    "California": "CA",
    "Colorado": "CO",
    "Connecticut": "CT",
    "Delaware": "DE",
    "Florida": "FL",
    "Georgia": "GA",
    "Hawaii": "HI",
    "Idaho": "ID",
    "Illinois": "IL",
    "Indiana": "IN",
    "Iowa": "IA",
    "Kansas": "KS",
    "Kentucky": "KY",
    "Louisiana": "LA",
    "Maine": "ME",
    "Maryland": "MD",
    "Massachusetts": "MA",
    "Michigan": "MI",
    "Minnesota": "MN",
    "Mississippi": "MS",
    "Missouri": "MO",
    "Montana": "MT",
    "Nebraska": "NE",
    "Nevada": "NV",
    "New Hampshire": "NH",
    "New Jersey": "NJ",
    "New Mexico": "NM",
    "New York": "NY",
    "North Carolina": "NC",
    "North Dakota": "ND",
    "Ohio": "OH",
    "Oklahoma": "OK",
    "Oregon": "OR",
    "Pennsylvania": "PA",
    "Rhode Island": "RI",
    "South Carolina": "SC",
    "South Dakota": "SD",
    "Tennessee": "TN",
    "Texas": "TX",
    "Utah": "UT",
    "Vermont": "VT",
    "Virginia": "VA",
    "Washington": "WA",
    "West Virginia": "WV",
    "Wisconsin": "WI",
    "Wyoming": "WY",
    "District of Columbia": "DC",
    "American Samoa": "AS",
    "Guam": "GU",
    "Northern Mariana Islands": "MP",
    "Puerto Rico": "PR",
    "United States Minor Outlying Islands": "UM",
    "U.S. Virgin Islands": "VI",
}
  return dick[value]

map_visualization_data = pd.read_csv("/content/investment (1).csv")
map_visualization_data["State Code"] = map_visualization_data["State"].apply(lambda x: translate(x))
map_visualization_data = map_visualization_data.rename(columns={"TotalAmountofAssistance": 'Total Amount of Assistance'})


fig = px.choropleth(map_visualization_data,
                    locations='State Code', 
                    locationmode="USA-states", 
                    scope="usa",
                    color='Total Amount of Assistance',
                    color_continuous_scale="purples"           
                    )
fig.show()

from sklearn.tree import export_graphviz
rmse = []
labels = pd.read_csv("/content/state_labels (3).csv", header=None)

training_data["Label"] = labels


working_training = pd.read_csv("/content/testing_data (1).csv")

clusters = clustering_by_label(working_training)



for cluster in clusters:
  cluster = cluster.drop(["Label"], axis = 1)
  X,y = X_y_Split(cluster, 3, -1, True)
  # print(X)
  # print(y)
  # clf = RandomForestClassifier(random_state=0)
  rmse_val = RandomForestRMSE(X,y)
  rmse.append(rmse)

from sklearn.tree import export_graphviz
import os
cluster = clusters[3]
cluster = cluster.drop(["Label"], axis = 1)
print(cluster["State"].unique())
X,y = X_y_Split(cluster, 3, -1, True)
clf = RandomForestClassifier(random_state=0)
pred_values = []
test_values = []
for n in range(6,len(X)+1,6):
    clf = RandomForestClassifier(random_state=0)
    training_x = X[n-6:n-1]

    testing_x = X[n-1]

    training_y = y[n-6:n-1]
    testing_y = y[n-1]
    test_values.append(testing_y)
    clf.fit(training_x,training_y)
    pred_value = clf.predict([testing_x])
    pred_values.append(int(pred_value))

X,y0 = X_y_Split(cluster, 3, -1, False)

print(len(pred_values), pred_values)
print(len(test_values), test_values)

Categories = cluster["State"].unique()[0:15]

df = pd.DataFrame(dict(index = range(0,len(Categories)), a = Categories, b = pred_values[0:15], c = test_values[0:15]))

plt.figure(figsize=(20,6))
# sns.stripplot(x="a", y="c", data = df, jitter=True, dodge=True, palette='viridis')
g = sns.scatterplot(data=df, x="index", y="b", color = "#8e40ed")
g = sns.scatterplot(data=df, x="index", y="c", color = "#5dbaf0")
g.set_xticks(range(len(df)))
g.set_xticklabels(Categories)
plt.legend(labels=['Predicted Values', 'Actual Values',])
plt.title("Predicted and Actual values for Cluster 2 in 2020")
plt.xlabel("States")
plt.ylabel("Total Assistance")

